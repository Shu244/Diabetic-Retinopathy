{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nimport csv\nimport os\nimport numpy as np\nimport random\ntf.enable_eager_execution()\nAUTOTUNE = tf.data.experimental.AUTOTUNE # Sets number of elements to prefetch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEfficientNetB0 - (224, 224, 3)\nEfficientNetB1 - (240, 240, 3)\nEfficientNetB2 - (260, 260, 3)\nEfficientNetB3 - (300, 300, 3)\n\nEfficientNetB4 - (380, 380, 3)\nEfficientNetB5 - (456, 456, 3)\nEfficientNetB6 - (528, 528, 3)\nEfficientNetB7 - (600, 600, 3)\n'''\n\n%cd ../input/efficientnet-repo\nimport efficientnet.efficientnet.tfkeras as efn\n%cd ../../working\n\ninput_shape = (256 , 256 , 3)\nweights_path = '../input/efficientnet-keras-weights-b0b5/efficientnet-b3_imagenet_1000_notop.h5'\nefn_model = efn.EfficientNetB3(weights=weights_path, include_top=False, input_shape=input_shape)\nprint('Model imported.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fine_tune_at = 523 # Start training on block 7 (see model.summary() for layer info)\n# for layer in model.layers[:fine_tune_at]:\n#     layer.trainable =  False\n# model.summary()\nefn_model.trainable = False\ndef build_model():\n    l2 = 0.00001\n    model = tf.keras.models.Sequential()\n    model.add(efn_model)\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dropout(rate=0.5))\n    model.add(tf.keras.layers.Dense(1200, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2)))\n    model.add(tf.keras.layers.Dropout(rate=0.5))\n    model.add(tf.keras.layers.Dense(500, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2)))\n    model.add(tf.keras.layers.Dropout(rate=0.5))\n    model.add(tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2)))\n    model.add(tf.keras.layers.Dropout(rate=0.5))\n    model.add(tf.keras.layers.Dense(5, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2)))\n    model.compile(\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        optimizer = tf.keras.optimizers.Adam(lr=0.0005),\n        metrics=['accuracy']\n    )\n    \n    return model\n\nmodel = build_model()\nprint('Model Built')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# '''\n# Reads entire preprocessed data into RAM.\n# '''\n# def load_data(image_folder_path, label_path):  \n#     '''\n#     Loading training and testing databases. \n#     '''\n    \n#     # Reading in labels and creating a dictionary\n#     reader = csv.reader(open(label_path, 'r')) # Reads in header as well but thats not important. \n#     label_dict = dict([(key, value) for key, value in reader])\n        \n#     image_tensors = []\n#     image_labels = []\n#     for name in os.listdir(image_folder_path):    \n#         image_path = os.path.join(image_folder_path, name) \n#         image_file = tf.read_file(image_path)\n#         image_tensor = tf.image.decode_png(image_file)\n#         image_tensors.append(image_tensor)\n        \n#         no_extension = name[:name.index('.')]\n#         image_labels.append(int(label_dict[no_extension]))\n        \n#         if len(image_tensors) == 2000:\n#             break\n    \n#     # Normalize image input. \n#     image_ds = tf.data.Dataset.from_tensor_slices(image_tensors)\n#     def normalize(image):\n#         return tf.cast(image/255,tf.float32) \n#     image_ds = image_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n    \n#     label_ds = tf.data.Dataset.from_tensor_slices(image_labels)\n#     ds = tf.data.Dataset.zip((image_ds, label_ds))\n       \n#     # Now determining how data is fed.\n#     ds = ds.shuffle(buffer_size=len(image_tensors)).repeat().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\n#     return ds, len(image_tensors)\n    \n# train_image_ds, train_ds_size = load_data('/kaggle/input/preprocessed/Preprocessed_Data', '/kaggle/input/aptos2019-blindness-detection/train.csv')\n# #test_image_ds = load_data('/kaggle/input/aptos2019-blindness-detection/test_images', '/kaggle/input/aptos2019-blindness-detection/test.csv')    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_FOLDER_PATH = '/kaggle/input/preprocessed-aug/Preprocessed_Data'\nLABEL_PATH = '/kaggle/input/aptos2019-blindness-detection/train.csv'\nTRAIN_BATCH_SIZE = 32\nVAL_BATCH_SIZE = 50\nEPOCHS = 50\nPERCENT_TRAIN = 0.8\n\ndef load_data(file_names, label_dict):\n    batch_shape = (len(file_names),) + input_shape\n    images = np.zeros(batch_shape)\n    labels = np.zeros(len(file_names))\n    i = 0\n    for file_name in file_names:    \n        image_path = os.path.join(IMAGE_FOLDER_PATH, file_name) \n        image_file = tf.read_file(image_path)\n        image_tensor = tf.image.decode_png(image_file)/255 # Normalize\n        images[i, ...] = image_tensor.numpy()\n        \n        no_extension = file_name[:file_name.index('.')]\n        if no_extension[0] == '~':\n            removed_tilda = no_extension[1:]\n            labels[i] = int(label_dict[removed_tilda])\n        else:\n            labels[i] = int(label_dict[no_extension])\n        \n        i = i + 1\n    \n    return images, labels\n\ndef image_loader(file_names, label_dict, batch_size):\n    L = len(file_names)\n    #this line is just to make the generator infinite, keras needs that    \n    while True:\n        random.shuffle(file_names)\n        batch_start = 0\n        batch_end = batch_size\n        while batch_start < L:\n            limit = min(batch_end, L)\n            images, labels = load_data(file_names[batch_start:limit], label_dict)\n\n            yield (images, labels) #a tuple with two numpy arrays with batch_size samples     \n\n            batch_start += batch_size   \n            batch_end += batch_size\n            \n# Reading in labels and creating a dictionary\nreader = csv.reader(open(LABEL_PATH, 'r')) # Reads in header as well but thats not important. \nlabel_dict = dict([(key, value) for key, value in reader])\n\nfile_names = os.listdir(IMAGE_FOLDER_PATH)\nrandom.shuffle(file_names)\nsplit_index = int(np.ceil(len(file_names)*PERCENT_TRAIN))\ntrain_file_names = file_names[:split_index]\nval_file_names = file_names[split_index:]\n\nprint('Data ready')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nTrain model\n'''\ncallbacks = [\n  # Interrupt training if `val_loss` stops improving for over 2 epochs\n  tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_loss'),\n  tf.keras.callbacks.ModelCheckpoint('model_weights', verbose=1, save_weights_only=True,period=3)\n]\n\nsteps_per_epoch = np.ceil(\n    float(\n        len(train_file_names)\n    )/TRAIN_BATCH_SIZE\n)\nvalidation_steps = np.ceil(\n    float(\n        len(val_file_names)\n    )/VAL_BATCH_SIZE\n)\nmodel.fit_generator(image_loader(train_file_names, label_dict, TRAIN_BATCH_SIZE), \n                    epochs=EPOCHS, \n                    steps_per_epoch=steps_per_epoch, \n                    callbacks=callbacks,\n                    validation_data=image_loader(val_file_names, label_dict, VAL_BATCH_SIZE),\n                    validation_steps=validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# '''\n# Custom training loop in order to avoid reading in entire training or testing dataset in RAM.\n# '''\n# BATCH_SIZE = 32\n# def load_data(image_folder_path, label_path):  \n#     '''\n#     Loading training and testing databases. \n#     '''\n    \n#     # Reading in labels and creating a dictionary\n#     reader = csv.reader(open(label_path, 'r')) # Reads in header as well but thats not important. \n#     label_dict = dict([(key, value) for key, value in reader])\n        \n#     image_paths = []\n#     image_labels = []\n#     for name in os.listdir(image_folder_path):    \n#         image_path = os.path.join(image_folder_path, name) \n#         image_paths.append(image_path)\n        \n#         no_extension = name[:name.index('.')]\n#         image_labels.append(int(label_dict[no_extension]))\n    \n#     image_paths_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n#     labels_ds = tf.data.Dataset.from_tensor_slices(image_labels)\n#     ds = tf.data.Dataset.zip((image_paths_ds, labels_ds))\n       \n#     # Now determining how data is fed.\n#     ds = ds.shuffle(buffer_size=len(image_paths)).repeat().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\n#     return ds, len(image_paths)\n\n# train_image_path_ds,_ = load_data('/kaggle/input/preprocessed/Preprocessed_Data', '/kaggle/input/aptos2019-blindness-detection/train.csv')\n# print('Data Loaded.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow import contrib\n# tfe = contrib.eager \n\n# NUM_EPOCHES = 300\n# SAVE_EVERY = 5 # Epoches.\n# SAVE_VERSIONS = 5\n# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n# global_step = tf.Variable(0) # Stores the number of learning steps taken.\n\n# def loss(inputs, labels):\n#     logits = model(inputs)\n#     return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n# def grad(inputs, labels):\n#     with tf.GradientTape() as tape:\n#         loss_value = loss(inputs, labels)\n#     return loss_value, tape.gradient(loss_value, model.trainable_variables)\n\n# def get_image(image_path):\n#     image_file = tf.read_file(image_path)\n#     image = tf.image.decode_png(image_file, channels=3)\n#     image = tf.cast(image/255,tf.float32)\n#     return image\n\n# save_version = 0\n# for epoch in range(NUM_EPOCHES):\n#     epoch_loss_avg = tfe.metrics.Mean()\n#     epoch_accuracy = tfe.metrics.Accuracy()\n\n#     # Training loop where one loop contains a mini-batch of elements.\n#     for image_paths, labels in train_image_path_ds:\n#         # Obtains image from path. \n#         images = np.array([])\n#         image_paths = image_paths.numpy()\n#         for image_path in image_paths:\n#             images = np.append(images, get_image(image_path))\n#         # Optimize the model\n#         loss_value, grads = grad(images, labels)\n#         optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\n\n#         # Track progress\n#         epoch_loss_avg(loss_value)  # add current batch loss\n#         # compare predicted label to actual label\n#         epoch_accuracy(model(image), y)\n\n#     # end epoch\n#     train_loss_results.append(epoch_loss_avg.result())\n#     train_accuracy_results.append(epoch_accuracy.result())\n\n#     print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(),epoch_accuracy.result()))\n    \n#     if epoch + 1 % SAVE_EVERY == 0:\n#         save_version = save_version % SAVE_VERSIONS\n#         model.save_weights('model_weights' + str(save_version) + '.h5', save_format='h5')\n        \n# '''\n# Still need to create new model that excludes optimizers. \n# '''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}